{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acdOpzV0lYwO"
      },
      "source": [
        "# A3: Data Preprocessing and Knowledge Discovery (20 points)\n",
        "\n",
        "### DUE: Sunday, 29 October 2023, 23:59\n",
        "\n",
        "## General Task Description\n",
        "\n",
        "This Jupyter notebook guides you through crucial steps in data preprocessing, which you will need for your IVDA projects. This includes handling missing values, grouping, mapping, normalizing data, and other key skills. The lecture content you'll need to help you through this assignment can be found in the L08 ppt file posted on OLAT. This Jupyter notebook is meant to help you along a self-study of that lecture material, which you'll begin during the usual lecture period on Thursday, October 18th, 14-16h. Please use this time to ensure that you're properly set up for working on the tasks presented. The TAs will be available to support you in this during the lecture period.\n",
        "\n",
        "If you have never used a Jupyter Notebook before, don't fret! Start-up instructions can be found here:\n",
        "* General, comprehensive walkthrough: https://www.dataquest.io/blog/jupyter-notebook-tutorial/ https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/\n",
        "* Jupyter Notebooks for VS Code users: https://code.visualstudio.com/docs/datascience/jupyter-notebooks \n",
        "\n",
        "The dataset we'll be wrangling today is from a publicly available questionnaire. The link to the questionnaire, which includes documentation on every question in your dataset, can be found here: https://data.mendeley.com/datasets/88y3nffs82/5/files/c5f122b0-0380-42e7-9930-3d85ee083a06.\n",
        "\n",
        "## Point Distribution: \n",
        "This assignment is worth 20 points in total. Your assigned tasks and questions can be found in **bold text** in the code skeleton below. Some tasks require a written response in addition to your code contribution. Generally, the points are allocated as follows, with a more detailed breakdown presented in the tasks themselves:\n",
        "\n",
        "* Task 1 - Handling duplicates & mapping Data (3 points)\n",
        "* Task 2 - Handling and treating missing values (4.5 points)\n",
        "* Task 3 - Grouping data & colour coding (2.5 points)\n",
        "* Task 4 - Normalizing data & colour coding (3 points)\n",
        "* Task 5 - KDD (7 points)\n",
        "\n",
        "## Submission: \n",
        "Please submit a PDF export of your completed Jupyter notebook, with all the cells' outputs visible, **especially the visualiations and written responses**. You will be graded on the outcome of your code and on your written responses, not the code itself. \n",
        "\n",
        "In your submission, please make sure you keep in mind the following:\n",
        "\n",
        "*  **Plots:** Always add titles, legends, axes descriptions, etc. It is recommended you use the *plotly* library.\n",
        "*  **Written responses:** Please answer as concisely as possible, while still using full sentences.\n",
        "*  **Allowed packages:** Your required packages are included in the code skeleton below. **Please do not import additional packages into this notebook**. Also, please do not use the *pandas-profiling* function. Your goal is to learn the abilities and limits of the tools presented below, to make you knowledgeable about the iterative process of data wrangling. However, you're welcome to use it to confirm your findings, if you so desire (not mandatory).\n",
        "* Selected solutions will be discussed after the deadline. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j8O2xUbk1JN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD5-yhCJtQw5"
      },
      "source": [
        "Import the data from the url:  https://raw.githubusercontent.com/akkuebler/ivda_dataprocessing/main/preppedStudentCovidData.csv'\n",
        "This is also provided to you in the A3 folder you've downloaded from OLAT, in case you have difficulties importing from a link.\n",
        "\n",
        "Note: you will want to have the responding student's number as your dataframe's index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1tPIH6tQw5"
      },
      "outputs": [],
      "source": [
        "#url =''\n",
        "#df = pd.read_csv ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0xZ21iFtQw5"
      },
      "source": [
        "Display the first 5 rows of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGHp8Zt4tQw6"
      },
      "outputs": [],
      "source": [
        "# display first rows\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aft-VH96tQw6"
      },
      "source": [
        "# Task 1 - Handling duplicates & mapping data (3 points)\n",
        "## Task 1.1 \n",
        "**Make yourself familiar with the data's shape and size**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYVxMwD6tQw7"
      },
      "outputs": [],
      "source": [
        "print(\"The dataset contains {} data records and {} features.\".format(df.shape[0], df.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7eRT6vwtQw7"
      },
      "source": [
        "Read through all the tasks in this notebook, and then create a list of all the questions in the questionaire which are relevant for your assigned data analysis tasks. \n",
        "\n",
        "Note: the answer to each question is stored in a column. In our context of A3, we refer to those columns as *attributes* or *features*\n",
        "\n",
        "## Task 1.2\n",
        "**Give basic summary statistics for the questions you've identified as relevant, and state which of these questions has the most missing data. (0.5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD1wUU-BtQw8"
      },
      "outputs": [],
      "source": [
        "# summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzW8IBoMxPkK"
      },
      "outputs": [],
      "source": [
        "# Which question has the most missing data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNMoxRrVtQw8"
      },
      "source": [
        "## Task 1.3\n",
        "When working with data, we should be concerned not just by missing data, but also by duplicated rows. **Check your dataset of relevant questions (from the above step) for duplicates, and delete all but the first entry.**\n",
        "\n",
        "Hint: Be aware that the dataset contains duplicate indices. The indices function as a unique student number for every student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvH5anCLtQw8"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av2vzQFntQw9"
      },
      "source": [
        "**Why can you not simply use `df.drop_duplicates(keep ='first')`? What would happen in this case? (Max. 50 words).** **(0.5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBALYRaJtQw9"
      },
      "outputs": [],
      "source": [
        "print(\"After handling duplicates, the dataset now contains {} data records and {} features.\".format(df.shape[0], df.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "YOUR RESPONSE HERE\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5GuWtlDtQw9"
      },
      "source": [
        "## Task 1.4\n",
        "**Create a pie chart visualization describing the ratio of responses from each country in your dataset. For now, do not perform any additional clean-up of your data. (0.5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mdivCAptQw-"
      },
      "outputs": [],
      "source": [
        "c = list(df[\"Q1\"])\n",
        "def counting_as_dict(c):\n",
        "    ###################################################\n",
        "    ##### YOUR CODE STARTS HERE #######################\n",
        "    ###################################################\n",
        "    pass\n",
        "    ###################################################\n",
        "    ##### YOUR CODE ENDS HERE #########################\n",
        "    ###################################################\n",
        "    \n",
        "    \n",
        "\n",
        "def plot_pie(dictionary):\n",
        "    ###################################################\n",
        "    ##### YOUR CODE STARTS HERE #######################\n",
        "    ###################################################\n",
        "    pass\n",
        "    ###################################################\n",
        "    ##### YOUR CODE ENDS HERE #########################\n",
        "    ###################################################\n",
        "\n",
        "#counting_as_dict(c)\n",
        "#plot_pie(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq-CoiSTtQw-"
      },
      "source": [
        "As you can see, the plot is very cluttered! To reduce this clutter, we could instead create and plot with a set of logical groupings-- in the case of our data here, these groupings could be continents. \n",
        "\n",
        "**Map the countries to the respective continents using the provided `continent.csv` file, and then plot it again. You can find this csv either in your A3 assignment folder from OLAT, or at https://raw.githubusercontent.com/akkuebler/ivda_dataprocessing/main/continents.csv** **(0.5 points)**\n",
        "\n",
        "Hint: Turn the `continents` dataframe into a dictionary you can use for mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k7UB7bvtQw-"
      },
      "outputs": [],
      "source": [
        "#url2 = \n",
        "#continents = pd.read_csv ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRH80FxEtQw_"
      },
      "outputs": [],
      "source": [
        "# Mapping \n",
        "\n",
        "\n",
        "# Visualization\n",
        "\n",
        "#counting_as_dict()\n",
        "#plot_pie()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Enz1C51tQw_"
      },
      "source": [
        "**Inspect the pie chart and the dataframe: Why do you see some 4-5% missing values? Where do these rows come from? (Max. 50 words). (0.5 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drgVeSV8_LRm"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "YOUR RESPONSE HERE\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlnbl2XttQw_"
      },
      "source": [
        "**Apply your findings from the question above to further reduce the amount of missing data. Plot the pie chart again. (0.5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsT7fTFKtQw_"
      },
      "outputs": [],
      "source": [
        "#Further reduce missing data\n",
        "\n",
        "\n",
        "\n",
        "# Map and visualize again\n",
        "\n",
        "#counting_as_dict()\n",
        "#plot_pie()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN1KzaSStQw_"
      },
      "source": [
        "*Please note: from the perspective of visuzalization theory, pie charts are not ideal!* This is because humans do not do well when asked to estimate quantities within (and across) pie charts with spatial disarray. As Edward Tufte once wrote: \n",
        "\n",
        "    \"... the only worse design than a pie chart is several of them.‚Äù \n",
        "    \n",
        "    Tufte. (2015). The visual display of quantitative information (Second edition, ninth printing). Graphics Press.\n",
        "\n",
        "In our context, we use pie charts only for the purpose of visualizing the progress we make in our grouping of the data. Since we do not set out to express some subtle findings to a reader, our use case is sufficiently simple for a pie chart. However, please not use pie charts in your upcoming group projects, or reports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJc9g2h0tQxA"
      },
      "source": [
        "# Task 2 - Handling and treating missing values (4.5 points)\n",
        "## Task 2.1\n",
        "**Visualize the age of the students in your dataset with an appropriate boxplot and histogram. With regards to the visualizations' treatment of missing data, what issues do you encounter here? (Max. 50 words) ( 1 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW0fSwfc38U8"
      },
      "outputs": [],
      "source": [
        "# Visualize "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "YOUR RESPONSE HERE\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm5PL5Wf383B"
      },
      "source": [
        "**Now, filter out missing data using `np.isnan()`, and run your visualization code again. (0.5 points)**\n",
        "\n",
        "Hint: If you are working on your local machine, you might want use `from plotly.offline import plot` together with `plot(fig)`. This creates a locally-stored HTML that is then opened within your web browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBq9ZF9xtQxA"
      },
      "outputs": [],
      "source": [
        "# Filter data using np.isnan \n",
        "\n",
        "# Visualize \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voVJf12ztQxA"
      },
      "source": [
        "## Task 2.2\n",
        "Inspect your dataset again. We still have to deal with some missing data!\n",
        " \n",
        "Now you must come up with a strategy to reduce your (reduced) dataset's missing values, so that you can continue your work on a nice, clean dataset.\n",
        "\n",
        "Hint: In general, there are 3 different ways to handle missing data: \n",
        "1. Ignoring on purpose (as we did above)\n",
        "2. Deleting entire entries (rows) or features (columns) with missing data, or deleting entire features when missing data reaches a certain threshold (i.e. ignoring them in your analysis)\n",
        "3. Imputing missing data with the feature's mean or median, or treat a missing value as seperate category (when the feature is categorical).\n",
        "\n",
        "But remember: All of the described ways above are data type- and context-dependent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuNv2o2HtQxA"
      },
      "source": [
        "## Task 2.2.1\n",
        "**Drop rows where all data is missing. Note that, since our index is the unique student number, we should NOT reset the index here. (0.5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOJCp-IHtQxA"
      },
      "outputs": [],
      "source": [
        "print(\"The dataset before dropping the rows contains {} data records and {} features.\".format(df.shape[0], df.shape[1]))\n",
        "\n",
        "# drop rows with missing data\n",
        "\n",
        "print(\"The dataset now contains {} data records and {} features.\".format(df.shape[0], df.shape[1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niZ-U2Z_tQxB"
      },
      "source": [
        "## Task 2.2.2\n",
        "This task is specifically about Q7 (Age).\n",
        "\n",
        "**Impute the missing values with the feature mean, and visualize again the age of the students in your dataset with an appropriate boxplot and histogram. (0.5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we1Z6c-PtQxB",
        "outputId": "e1b031b6-299c-460c-f5d1-ea6aec36e57d"
      },
      "outputs": [],
      "source": [
        "print(\"Original Data : \\n\", df.Q7.describe(), \"\\n\")\n",
        "\n",
        "\n",
        "# impute missing values\n",
        "print(\"Imputed Data : \\n\", df.Q7.describe())\n",
        "\n",
        "\n",
        "# Visualize \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L72UI6n9tQxB"
      },
      "source": [
        "**Show the graphs with the imputed and ignored datasets in one plot object. Model your visualization after the example below, and annotate each approach's mean, so that easy comparison between approaches is possible. In total, there should be 2 histograms and 2 boxplots (1 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OIfjJqYtQxB"
      },
      "source": [
        "[IMAGE](https://github.com/akkuebler/ivda_dataprocessing/blob/main/annotation_means_%20example.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laQNzmGBtQxC"
      },
      "outputs": [],
      "source": [
        "# Visualize \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFurwujktQxC"
      },
      "source": [
        "**Shortly describe the graph. What can you infer from it? Do the boxplots and histograms differ? If so, explain why. (Max. 100 words).** **(0.5 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVhfnLNj-88a"
      },
      "source": [
        "    YOUR RESPONSE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5nhXg3mtQxC"
      },
      "source": [
        "## Task 2.2.3\n",
        "This task is about Q4 (Student Status).\n",
        "\n",
        "**As this is categorical data, you will form a new category for the missing data and name it *'Unanswered'*.\n",
        "Then, create a plot for each age group stated below, describing the proportion of students who have attended school full-time and part-time. (0.5 points)**\n",
        "\n",
        "Age groups to include:\n",
        "* 18-28\n",
        "* 29-38\n",
        "* 39+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVUmAb0-tQxC"
      },
      "outputs": [],
      "source": [
        "# Form new category named \"Unanswered\"\n",
        "\n",
        "# Visualize \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c2vxV8HtQxC"
      },
      "source": [
        "\n",
        "# Task 3 - Grouping data & colour coding (2.5 points)\n",
        "In this task, we will visualize students' perception of how their workload has changed since in-person classes were cancelled. Group the data depending on whether the student is a full-time or a part-time student. The associated questions in the questionaire are Q17 and Q4.\n",
        "\n",
        "**First, take care of the missing data present in this set of responses, and describe your strategy below (Max. 50 words) (0.5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VY4mhJptQxC"
      },
      "outputs": [],
      "source": [
        "# Handle missing data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgWEsILmB6Ep"
      },
      "source": [
        "    \n",
        "\n",
        "```\n",
        "YOUR RESPONSE HERE\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5sFOMKDCGz4"
      },
      "source": [
        "**Use an appropriate stacked bar chart as visualization. Plot it two times, once with a two-sided gradient color scheme in red/green, and the other time using the following colours: https://coolors.co/a4161a-e5383b-ffffff-f5f3f4-b1a7a6. (1.5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Engt5vCgCGXC"
      },
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0twwscG_Ch4X"
      },
      "source": [
        "**Discuss the pros and cons of the second colour scheme, and give an example of when it may not be appropriate to use. (Max. 50 words)**  **(0.5 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GLfMYz5CPHI"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "YOUR RESPONSE HERE\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLoqIUjxtQxD"
      },
      "source": [
        "# Task 4 - Normalizing data & colour coding (3 points)\n",
        "In this task, you will build your own \"emotional satisfaction score\" (ESS) using the data from Q25. \n",
        "\n",
        "To do this, you will group the data into positive and negative feelings, sum up the relevant features, and then normalize the results to a scale of 0 to 1. Use the value for the given categorical data item in Q25 as a measure of \"emotional intensity\" (i.e. 'Never'= 1 and 'Always'= 5). Our \"emotional satisfaction score\" attempts to balance out positive and negative emotions, and so you will calculate it using the formula: \n",
        "\n",
        "    ESS = (positive emotions score sum) - (negative emotions score sum) \n",
        "\n",
        "Afterwards, use an appropriate colour-coded plot to show differences between Bachelor, Masters, and PhD students, using responses from Q5.\n",
        "\n",
        "**Plot: 2 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgQ6K5KjtQxD"
      },
      "outputs": [],
      "source": [
        "print(\"The dataset contains {} data records and {} features.\".format(df.shape[0], df.shape[1]))\n",
        "\n",
        "# Handling missing data\n",
        "\n",
        "print(\"The dataframe after removing rows with NaN value in the specified columns contains {} data records and {} features.\".format(df_work_emotion.shape[0], df_work_emotion.shape[1]))\n",
        "\n",
        "# Summarize specific columns\n",
        "\n",
        "\n",
        "# Normalize new columns\n",
        "\n",
        "\n",
        "# Calculate final emotional satisfaction score\n",
        "\n",
        "\n",
        "# Group by Q5, calculate average \n",
        "\n",
        "\n",
        "# Visualization \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQwBgdAOFERL"
      },
      "source": [
        "**Discuss your visualized results. First, describe how you treated missing data here. Second, comment on whether our initial assumption, that the negative emotions experienced by students are ultimately balanced out by their positive emotions, applies to any of the groups we visualized. If so, for which group is this the case?  (Max. 100 words)** **(1 point)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kglbiRQsFptS"
      },
      "source": [
        "    YOUR RESPONSE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5 - KDD (7 points)\n",
        "Taking into account the KDD process you recently learned in the lecture, we now focus on data mining. Imagine the EU wants to help the group with the worst ESS. \n",
        "\n",
        "**Use an unsupervised learning approach to help universities target their interventions appropriately. An outline of the steps you must cover is presented below:**\n",
        "\n",
        "1. Consider the answers from Q26, about worries on personal circumstances, and filter for European countries only, using the steps you've practiced above. Do not forget to also filter for the group with the lowest ESS as an outcome of Task 4. \n",
        "2. Use an appropriate clustering model (K-Means, Hierarchical Clustering, DBSCAN, etc.). You can find documentation on how to implement this models at: https://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning\n",
        "3. **Visualize the results of the KDD process. Justify your visualization encoding choices using the presented course methodologies (Max. 150 words, see slides and textbook). (3 points)**\n",
        "4. **Interpretation/Evaluation: Describe your KDD approach, focusing on answering these topics (Max. 250 words) (4 points):**\n",
        " * Can the students can be grouped into distinct clusters based on their worries about personal circumstances? If you've found that this is the case, what are the characteristics of these clusters?\n",
        " * What patterns can you detect in your model's results?\n",
        " * Can we identify segments of students who are particularly at high risk, due to pervasive worries across all categories?\n",
        " * What recommendation would you give to the universities, based on your visualized results?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "YOUR RESPONSE HERE\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
